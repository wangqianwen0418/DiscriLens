{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "\n",
    "import random\n",
    "import re\n",
    "import copy\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_names = (\n",
    "    \"checking_status\",\n",
    "    \"duration\",\n",
    "    \"credit_history\",\n",
    "    \"purpose\",\n",
    "    \"credit_amount\",\n",
    "    \"savings_status\",\n",
    "    \"employment\",\n",
    "    \"installment_commitment\",\n",
    "    \"personal_status\",\n",
    "    \"other_parties\",\n",
    "    \"residence_since\",\n",
    "    \"property_magnitude\",\n",
    "    \"age\",\n",
    "    \"other_payment_plans\",\n",
    "    \"housing\",\n",
    "    \"existing_credits\",\n",
    "    \"job\",\n",
    "    \"num_dependents\",\n",
    "    \"own_telephone\",\n",
    "    \"foreign_worker\",\n",
    "    \"class\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# df = pandas.read_table('./german.data-numeric.txt', delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_range_cols(data):\n",
    "    range_col = [] # columns that fit numerical values into ranges\n",
    "    for col in data.select_dtypes(exclude=['int64']).columns:\n",
    "        if any(item in data[col][0] for item in ['>', '<']): \n",
    "            range_col.append(col)\n",
    "                \n",
    "    return range_col\n",
    "\n",
    "class DataEncoder(object):\n",
    "    def __init__(self, class_column='class', cat_columns=None):\n",
    "        self.class_column = class_column\n",
    "        self.cat_columns = cat_columns\n",
    "\n",
    "        # these will be trained with fit_encoders()\n",
    "        self.column_encoders = {} # label encoder\n",
    "        self.cat_encoder = None # one-hot encoder\n",
    "        self.label_encoder = None # label encoder\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"\n",
    "        Fit one-hot encoders for categorical features and an integer encoder for\n",
    "        the label. These can be used later to transform raw data into a form\n",
    "        that ATM can work with.\n",
    "\n",
    "        data: pd.DataFrame of unprocessed data\n",
    "        \"\"\"\n",
    "        if self.class_column not in data.columns:\n",
    "            raise KeyError('Class column \"%s\" not found in dataset!' %\n",
    "                           self.class_column)\n",
    "            \n",
    "        range_col = find_range_cols(data)\n",
    "                \n",
    "        self.range_col = range_col\n",
    "            \n",
    "\n",
    "        # encode categorical columns, leave ordinal values alone\n",
    "        if self.cat_columns is None:\n",
    "            cats = data.drop([self.class_column]+range_col, axis=1).select_dtypes(exclude=['int64'])\n",
    "            self.cat_columns = cats.columns\n",
    "        else:\n",
    "            cats = data[self.cat_columns].drop(range_col, axis=1).select_dtypes(exclude=['int64'])\n",
    "            \n",
    "        self.cat_cols = cats.columns\n",
    "        \n",
    "        for cat_name in cats.columns:   \n",
    "        # save the indices of categorical columns for one-hot encoding\n",
    "\n",
    "            # encode each feature as an integer in range(unique_vals)\n",
    "            le = LabelEncoder()\n",
    "            cats[cat_name] = le.fit_transform(cats[cat_name])\n",
    "            self.column_encoders[cat_name] = le\n",
    "\n",
    "        # One-hot encode the whole feature matrix.\n",
    "        # Set sparse to False so that we can test for NaNs in the output\n",
    "        self.cat_encoder = OneHotEncoder(sparse=False)\n",
    "        self.cat_encoder.fit(cats)\n",
    "\n",
    "        # Train an encoder for the label as well\n",
    "        labels = np.array(data[[self.class_column]])\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(labels)\n",
    "        \n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"\n",
    "        Convert a DataFrame of labeled data to a feature matrix in the form\n",
    "        that ATM can use.\n",
    "        \"\"\"\n",
    "        y = self.transform_y(data)\n",
    "        X = self.transform_x(data)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    def transform_x(self, data):\n",
    "        \"\"\"\n",
    "        only transform x, for the generated data\n",
    "        \"\"\"\n",
    "        cats = data[self.cat_columns]\n",
    "\n",
    "        # encode each categorical feature as an integer\n",
    "        for column, encoder in list(self.column_encoders.items()):\n",
    "            cats[column] = encoder.transform(cats[column])\n",
    "\n",
    "        # one-hot encode the categorical features\n",
    "        X = self.cat_encoder.transform(cats)\n",
    "        \n",
    "        if self.class_column in data:\n",
    "            nums = data.drop([self.class_column], axis=1).select_dtypes(include=['int64']).values\n",
    "        else:\n",
    "            nums = data.select_dtypes(include=['int64']).values\n",
    "            \n",
    "       \n",
    "        # transform range cols into integrate. e.g., <4 -> 1; 4<x<7 -> 2\n",
    "        ranges = []\n",
    "        for col in self.range_col:\n",
    "            values = data[col]\n",
    "            ranges.append( self.range2int(values) )\n",
    "        ranges = np.transpose( np.array(ranges) )\n",
    "#         print(X.shape, nums.shape, ranges.shape)\n",
    "        \n",
    "        X = np.concatenate((X, nums, ranges), axis=1)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def transform_y(self, data):\n",
    "        if self.class_column in data:\n",
    "            # pull labels into a separate series and transform them to integers\n",
    "            labels = np.array(data[[self.class_column]])\n",
    "            y = self.label_encoder.transform(labels)\n",
    "            # drop the label column and transform the remaining features\n",
    "        else:\n",
    "            y = None\n",
    "            \n",
    "        return y\n",
    "    \n",
    "    def range2int(self, values):\n",
    "        ranges = []\n",
    "        for v in values:\n",
    "            if v not in ranges:\n",
    "                ranges.append(v)\n",
    "        \n",
    "        def sort_key(x):\n",
    "            num_strings = re.findall('\\d+', x)\n",
    "            # 'undefined' is in the front\n",
    "            if len(num_strings)==0:\n",
    "                return -1\n",
    "            # x> 1, x<7\n",
    "            elif len(num_strings)==1:\n",
    "                return int(num_strings[0])*2\n",
    "            # 1<x<7\n",
    "            else:\n",
    "                nums = map(int, num_strings) # string to number\n",
    "                return sum(nums)\n",
    "                \n",
    "        ranges.sort(key=sort_key)\n",
    "        return list(map(lambda x: ranges.index(x), values))\n",
    "        \n",
    "        \n",
    "    \n",
    "    def fit_transform(self, data):\n",
    "        \"\"\" Process data into a form that ATM can use. \"\"\"\n",
    "        self.fit(data)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'categories'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8f491737e935>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-dbf8f08f3353>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;31m# One-hot encode the whole feature matrix.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;31m# Set sparse to False so that we can test for NaNs in the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat_encoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'categories'"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv('./credit_clean.csv')\n",
    "\n",
    "# drop rows with any NA values\n",
    "data = data.dropna(how='any')\n",
    "\n",
    "encoder = DataEncoder()\n",
    "encoder.fit(data)\n",
    "x_train, y_train = encoder.transform(data)\n",
    "x_train\n",
    "y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_cls = SVC(\n",
    "    C = 0.0484055070919,\n",
    "    cache_size = 15000,\n",
    "    class_weight = \"balanced\",\n",
    "    gamma = 424.665365448,\n",
    "    kernel = \"rbf\",\n",
    "    max_iter = 50000,\n",
    "    probability = True,\n",
    "    shrinking = True)\n",
    "\n",
    "\n",
    "mlp_cls = MLPClassifier(\n",
    "    activation = \"relu\",\n",
    "    alpha = 0.00146693288878,\n",
    "    batch_size = \"auto\",\n",
    "    hidden_layer_sizes= (189,50, ),\n",
    "    learning_rate = \"constant\",\n",
    "    learning_rate_init = 0.898892816545,\n",
    "    solver = \"sgd\"\n",
    ")\n",
    "\n",
    "rf_cls = RandomForestClassifier(\n",
    "    bootstrap=True, \n",
    "    class_weight=None, \n",
    "    criterion='entropy', \n",
    "    max_depth=10, \n",
    "    max_features=0.45, \n",
    "    max_leaf_nodes=None, \n",
    "    min_impurity_decrease=1e-07, \n",
    "    min_samples_leaf=6, \n",
    "    min_samples_split=7, \n",
    "    min_weight_fraction_leaf=0.0, \n",
    "    n_estimators=512, \n",
    "    n_jobs=1, \n",
    "    oob_score=False, \n",
    "    random_state=3, \n",
    "    verbose=0, \n",
    "    warm_start=False\n",
    ")\n",
    "\n",
    "knn_cls = KNeighborsClassifier(\n",
    "    algorithm = \"ball_tree\",\n",
    "    leaf_size = 40,\n",
    "    metric = \"manhattan\",\n",
    "    n_neighbors = 17\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_score = cross_val_score(svm_cls, x_train, y_train, scoring='accuracy', cv=10) \n",
    "# mlp_score = cross_val_score(mlp_cls, x_train, y_train, scoring='accuracy', cv=10) \n",
    "\n",
    "knn_cls.fit(x_train, y_train)\n",
    "knn_score = cross_val_score(knn_cls, x_train, y_train, scoring='accuracy', cv=10) \n",
    "\n",
    "rf_cls.fit(x_train, y_train)\n",
    "rf_score = cross_val_score(rf_cls, x_train, y_train, scoring='accuracy', cv=10) \n",
    "\n",
    "print('knn', knn_score, 'rf', rf_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator\n",
    "class DataGene(object):\n",
    "    \n",
    "    def __init__(self, data, sample_num=10, class_col='class'):\n",
    "        self.data = data\n",
    "        self.sample_num = sample_num\n",
    "        self.class_col = 'class'\n",
    "        \n",
    "    def get_samples(self):\n",
    "        features = self.data.drop([self.class_col], axis=1)\n",
    "        range_cols = find_range_cols(self.data)\n",
    "        cat_cols = features.select_dtypes(exclude=['int']).drop(range_cols, axis=1).columns\n",
    "        num_cols = features.select_dtypes(include=['int']).columns\n",
    "        \n",
    "        samples = []\n",
    "        \n",
    "        for i in range(self.sample_num):\n",
    "            sample_cat = [\n",
    "                random.choice(encoder.column_encoders[cat_name].classes_)\n",
    "                for cat_name in cat_cols\n",
    "            ]\n",
    "            sample_num = [\n",
    "                random.choice( list(set(features[num_name])) )\n",
    "                for num_name in num_cols\n",
    "            ]\n",
    "            sample_range = [\n",
    "                random.choice( list(set(features[range_name])) )\n",
    "                for range_name in range_cols\n",
    "            ]\n",
    "            sample = sample_cat + sample_num + sample_range\n",
    "            samples.append(sample)\n",
    "            \n",
    "        \n",
    "        samples = pd.DataFrame(samples, columns=list(cat_cols)+list(num_cols)+range_cols)\n",
    "        return samples        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new samples\n",
    "dataGene = DataGene(data, sample_num=3000)\n",
    "new_samples = dataGene.get_samples()\n",
    "# encode sample data\n",
    "x_samples, y_samples = encoder.transform(new_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate samples to describe model behavior\n",
    "def add_col(df, new_col, col_name):\n",
    "    new_df = df.copy()\n",
    "    new_df[col_name] = pd.Series(np.asarray(new_col), index= df.index) \n",
    "    return new_df\n",
    "\n",
    "y_samples_knn = knn_cls.predict(x_samples)\n",
    "y_samples_rf = rf_cls.predict(x_samples)\n",
    "\n",
    "\n",
    "# pandas data frame\n",
    "knn_samples = add_col(new_samples, y_samples_knn, 'class') \n",
    "rf_samples = add_col(new_samples, y_samples_rf, 'class')\n",
    "\n",
    "model_samples = knn_samples\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find key featurea \n",
    "import re\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from pycausal import search as s\n",
    "\n",
    "pc = pc()\n",
    "pc.start_vm()\n",
    "\n",
    "\n",
    "\n",
    "def findKeyAttrs(samples, protect_attr, result_attr = 'class'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        samples(pandas data frames): \n",
    "        protect_attr(string || Array<string>): \n",
    "    Return:\n",
    "        key_attrs(list<string>): a list of key attributes that directly influence the decision\n",
    "    \"\"\"\n",
    "    key_attrs = []\n",
    "    ### use bayes Est to find the key attributes\n",
    "    ### slow, extract more key attributes \n",
    "#     graph = s.bayesEst(samples, depth = 0, alpha = 0.05, verbose = True)\n",
    "    ### OR use Fast Greedy Equivalence Search\n",
    "    ### faster than bayes, get less key attributes\n",
    "    graph = s.tetradrunner()\n",
    "    graph.getAlgorithmParameters(algoId = 'fges', scoreId = 'bdeu')\n",
    "\n",
    "    graph.run(algoId = 'fges', dfs = data, scoreId = 'bdeu', priorKnowledge = None, dataType = 'discrete',\n",
    "           structurePrior = 0.5, samplePrior = 0.5, maxDegree = 5, faithfulnessAssumed = True, verbose = False)\n",
    "    \n",
    "#     graph.getNodes()\n",
    "    \n",
    "    for edge in graph.getEdges():\n",
    "        if 'class' in edge:\n",
    "            # extract attr name from the edge\n",
    "            # remove --> or --o or --- and white space\n",
    "            attr = re.sub(r'-+>?o?|{}|\\s+'.format(result_attr), '', edge)\n",
    "            key_attrs.append(attr)\n",
    "            \n",
    "            \n",
    "    # remove protect attrs        \n",
    "    if type(protect_attr) is not str: \n",
    "        # if protect attr is a list\n",
    "        for a in protect_attr:\n",
    "            if a in key_attrs:\n",
    "                key_attrs.remove(a)\n",
    "    elif protect_attr in key_attrs:\n",
    "        # if protect attr is a string\n",
    "        key_attrs.remove(protect_attr)\n",
    "    print('key attributes', key_attrs)\n",
    "    return key_attrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify candidate groups\n",
    "protect_attr = 'age'\n",
    "protect_vals = list(set(model_samples[protect_attr]))\n",
    "print(protect_vals)\n",
    "\n",
    "key_attrs = findKeyAttrs(model_samples, protect_attr)\n",
    "key_vals = {}\n",
    "for key_attr in key_attrs:\n",
    "    key_vals[key_attr] = list(set(model_samples[key_attr]))\n",
    "    \n",
    "key_groups = []\n",
    "\n",
    "def generate_groups(key_vals, depth, index, key_groups):\n",
    "    '''\n",
    "    Args: \n",
    "        key_vals(dict): e.g., {'atrribute_a':['a', 'b', 'c'], 'attribute_b': ['m', 'n', 'k']}\n",
    "        depth(int): recursive depth\n",
    "        index(dict): the choosen value of each key attribute. e.g.,  {'atrribute_a':'a', 'attribute_b': 'k'}\n",
    "        key_groups: a list of index\n",
    "    \n",
    "    '''\n",
    "    print('a', key_vals,key_attrs, depth )\n",
    "    for k in key_vals[ key_attrs[depth] ]:\n",
    "        index_ = copy.deepcopy(index)\n",
    "        index_[key_attrs[depth]] = k\n",
    "        if depth < len(key_attrs)-1: \n",
    "            depth_ = depth+1 \n",
    "            generate_groups(key_vals, depth_, index_, key_groups)\n",
    "        else:\n",
    "            key_groups.append(index_)\n",
    "    \n",
    "\n",
    "generate_groups(key_vals, 0, {}, key_groups )\n",
    "# print(key_groups)\n",
    "\n",
    "for group in key_groups:\n",
    "    group_items = model_samples.copy()\n",
    "    for attr in group:\n",
    "        group_items = group_items.loc[group_items[attr]==group[attr]]\n",
    "    print(group, len(group_items))\n",
    "    if len(group_items)>0:\n",
    "        for val in protect_vals:\n",
    "            # based on protected attribute\n",
    "            group_items_ = group_items.loc[group_items[protect_attr] == val]\n",
    "            if len(group_items)>0:\n",
    "                group_reject = group_items_.loc[group_items_['class'] == 0]\n",
    "                group_accept = group_items_.loc[group_items_['class'] == 1]\n",
    "                p_0 = len(group_reject)/len(group_items_)\n",
    "                p_1 = len(group_accept)/len(group_items_)\n",
    "                print(val, \"{:.2f}\".format(p_0), \"{:.2f}\".format(p_1), len(group_items_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization using vega lite\n",
    "from vega import VegaLite\n",
    "\n",
    "VegaLite({\n",
    "  \"$schema\": \"https://vega.github.io/schema/vega-lite/v3.json\",\n",
    "  \"transform\":[\n",
    "    {\"calculate\": \"datum.class == 1 ? 'approve' : 'reject'\", \"as\": \"result\"},\n",
    "    {\n",
    "      \"aggregate\": [\n",
    "        {\n",
    "          \"op\": \"count\",\n",
    "          \"as\": \"count_sex\"\n",
    "        }\n",
    "      ],\n",
    "      \"groupby\": [\n",
    "        \"sex\",\n",
    "        \"result\"\n",
    "      ]\n",
    "    },\n",
    "    {\"stack\": \"count_sex\", \"as\": [\"f\", \"m\"], \"groupby\": [\"result\"]},\n",
    "    {\n",
    "      \"window\": [\n",
    "        {\n",
    "          \"field\": \"sex\",\n",
    "          \"op\": \"count\",\n",
    "          \"as\": \"offset\"\n",
    "        }\n",
    "      ],\n",
    "      \"groupby\": [\"result\"],\n",
    "      \"frame\": [\"null\", \"null\"]\n",
    "    },  \n",
    "    {\"calculate\": \"datum.f-datum.offset/2\", \"as\": \"yf\"},\n",
    "    {\"calculate\": \"datum.m-datum.offset/2\", \"as\": \"ym\"}\n",
    "  ],\n",
    "  \"mark\": \"bar\",\n",
    "  \"encoding\": {\n",
    "    \"x\": {\"type\": 'nominal', \"field\": \"result\"},\n",
    "#     \"y\": {\"aggregate\": \"count\", \"field\": \"sex\", \"type\": \"quantitative\"},\n",
    "    \"y\": {\"field\": \"yf\",\"type\": \"quantitative\"},\n",
    "    \"y2\": {\"field\": \"ym\",\"type\": \"quantitative\"},\n",
    "    \"color\": {\n",
    "        \"field\": \"sex\", \n",
    "        \"type\": \"nominal\",\n",
    "        \"scale\": {\"range\": [\"#F58518\", \"#4C78A8\"]}\n",
    "    },\n",
    "  }\n",
    "}, samples_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viusalization using py_d3\n",
    "\n",
    "%load_ext py_d3\n",
    "\n",
    "%%d3\n",
    "<style>\n",
    "\n",
    ".bar {\n",
    "  fill: steelblue;\n",
    "}\n",
    "\n",
    ".bar:hover {\n",
    "  fill: brown;\n",
    "}\n",
    "\n",
    ".axis--x path {\n",
    "  display: none;\n",
    "}\n",
    "\n",
    "</style>\n",
    "<svg width=\"960\" height=\"500\"></svg>\n",
    "<script>\n",
    "\n",
    "var svg = d3.select(\"svg\"),\n",
    "    margin = {top: 20, right: 20, bottom: 30, left: 40},\n",
    "    width = +svg.attr(\"width\") - margin.left - margin.right,\n",
    "    height = +svg.attr(\"height\") - margin.top - margin.bottom;\n",
    "\n",
    "var x = d3.scaleBand().rangeRound([0, width]).padding(0.1),\n",
    "    y = d3.scaleLinear().rangeRound([height, 0]);\n",
    "\n",
    "var g = svg.append(\"g\")\n",
    "    .attr(\"transform\", \"translate(\" + margin.left + \",\" + margin.top + \")\");\n",
    "\n",
    "d3.tsv(\"../data/data.tsv\", function(d) {\n",
    "  d.frequency = +d.frequency;\n",
    "  return d;\n",
    "}, function(error, data) {\n",
    "  if (error) throw error;\n",
    "\n",
    "  x.domain(data.map(function(d) { return d.letter; }));\n",
    "  y.domain([0, d3.max(data, function(d) { return d.frequency; })]);\n",
    "\n",
    "  g.append(\"g\")\n",
    "      .attr(\"class\", \"axis axis--x\")\n",
    "      .attr(\"transform\", \"translate(0,\" + height + \")\")\n",
    "      .call(d3.axisBottom(x));\n",
    "\n",
    "  g.append(\"g\")\n",
    "      .attr(\"class\", \"axis axis--y\")\n",
    "      .call(d3.axisLeft(y).ticks(10, \"%\"))\n",
    "    .append(\"text\")\n",
    "      .attr(\"transform\", \"rotate(-90)\")\n",
    "      .attr(\"y\", 6)\n",
    "      .attr(\"dy\", \"0.71em\")\n",
    "      .attr(\"text-anchor\", \"end\")\n",
    "      .text(\"Frequency\");\n",
    "\n",
    "  g.selectAll(\".bar\")\n",
    "    .data(data)\n",
    "    .enter().append(\"rect\")\n",
    "      .attr(\"class\", \"bar\")\n",
    "      .attr(\"x\", function(d) { return x(d.letter); })\n",
    "      .attr(\"y\", function(d) { return y(d.frequency); })\n",
    "      .attr(\"width\", x.bandwidth())\n",
    "      .attr(\"height\", function(d) { return height - y(d.frequency); });\n",
    "});\n",
    "\n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type('a')=='str'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in {'a':1, 'b':2}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
