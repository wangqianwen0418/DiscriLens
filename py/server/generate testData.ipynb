#%% Change working directory from the workspace root to the ipynb file location. Turn this addition off with the DataScience.changeDirOnImportExport setting
# ms-python.python added
import os
try:
	os.chdir(os.path.join(os.getcwd(), 'py/server'))
	print(os.getcwd())
except:
	pass

#%%

import os
import pandas as pd
import json
import numpy as np

from model import num2cate_fit, num2cate_transform, generate_samples, generate_model_samples, ModelGene, findKeyAttrs, FindGroups, get_numAttrs, find_rules
from model.samples import DataGene
from model.data_encoder import DataEncoder
from joblib import dump, load

from sklearn.model_selection import KFold
import dill
import pickle
import warnings
warnings.filterwarnings('ignore')

from itertools import chain


#%%


store_path = '../../front/src/testdata/'



    
    
def init_samples(dataset_name, find_key=False):
    sample_num = 1000 # number of generated data 
    dataset_path = '../server/{}.csv'.format(dataset_name)
    data = pd.read_csv(dataset_path)
    
    mdlp = num2cate_fit(data, 2)
    
    if find_key:
        # find key_attrs
        key_attrs = findKeyAttrs(num2cate_transform(data, mdlp))
        f = open(store_path + '{}_key.json'.format(dataset_name),'w')
        json.dump(key_attrs, f)
    
    #  save mdelp
    f=open('{}_mdlp.pkl'.format(dataset_name), 'wb')
    dill.dump(mdlp, f, -1)
    
    
    # generate samples
    samplesInit = generate_samples(data, sample_num)
    samples_path = os.path.join(store_path, '{}_samples.json'.format(dataset_name))
    samplesInit.to_json(samples_path, orient='records')


def get_model_samples(dataset_name, models=['lr', 'knn', 'xgb'], protect_attr=''):
    """
    train model on the training data,
    return the generated samples based on the training data
    """
    if os.path.isfile('{}_mdlp.pkl'.format(dataset_name)):
        f=open('{}_mdlp.pkl'.format(dataset_name), 'rb')
        mdlp = dill.load(f)
    else:
        raise Exception('no mdlp exists, run init_samples first')
        
    dataset_path = '../server/{}.csv'.format(dataset_name)
    data = pd.read_csv(dataset_path)
    
    

    samples_path = os.path.join(store_path, '{}_samples.json'.format(dataset_name))
    samplesInit = pd.read_json(samples_path)
    
    


    
    for var in models:
        
        # set name
        model_name = dataset_name+"_"+var
        model_gene = ModelGene(model_name)
    
        # train model
        model, encoder, score = model_gene.fit_model( num2cate_transform(data, mdlp) )
        f = open('./cache/models/model_{}_{}.pkl'.format(var, dataset_name), 'wb')    
        pickle.dump(model, f)  
        
        # general samples
    
        num_samples, cate_samples = generate_model_samples(samplesInit, mdlp, model, encoder) 
        
        # add the ID col 
        cate_samples.insert(loc=0, column='id', value= cate_samples.index)
        num_samples.insert(loc=0, column='id', value= num_samples.index)
        
        # save mdeol & samples to cache
        dataOut = pd.concat([num_samples,cate_samples])
        
        samples_path = os.path.join(store_path, '{}_samples.json'.format(model_name))
        dataOut.to_json(samples_path, orient='records')

        samples_path = os.path.join(store_path, '{}_samples.json'.format(model_name))
        dataOut.to_json(samples_path, index=False)
        
        print(dataset_name + ' ' + var + ' accuracy: ' + str(score))
        
    print(dataset_name + ' all done')
    
def get_rules(dataset_name, protect_attr='',model_name=None, min_support = 5):
    
    model_name = '{}_{}'.format(dataset_name, model_name)
    sample_path = os.path.join(store_path, '{}_samples.json'.format(model_name))
    model_samples = pd.read_json(sample_path)
    model_samples = model_samples.iloc[int(len(model_samples)/2):]
    rules = find_rules(model_samples, minimum_support=min_support, min_len=1, protect_attr = protect_attr, target_attr='class', elift_th=[1, 1])

    
    rules.to_json(store_path + '{}_rules.json'.format(model_name),orient='records')

def get_all_rules(protect, models=['xgb','knn','lr'], dataset='adult'):
    
    for model in models:
        get_rules(dataset, protect, model)
        get_rules(dataset, protect, model)
        get_rules(dataset, protect, model)
    for model in models:
        sample_name = os.path.join(store_path, dataset + '_' + model + '_samples.json')
        rule_name = os.path.join(store_path, dataset + '_' + model + '_rules.json')
        samples = pd.read_json(sample_name)

        # add item id to rules
        def item_within_rule(item, rule_context):
            for attr_val in rule_context:
                attr, val = attr_val.split('=')
                if not item[attr] == val :
                    return False
            return True

        rules = pd.read_json(rule_name)
        rules['items'] = ''
        for idx, rule in rules.iterrows():
            rules.at[idx, 'items'] = [sample['id'] for i,sample in samples.iloc[int(len(samples)/2):].iterrows() if item_within_rule(sample, rule["antecedent"])] 
        rules.to_json(rule_name, orient='records')

        print(dataset+'_'+model+' has done')

    print('All finished')


#%%
models =['svm', 'lr', 'knn', 'dt', 'rf', 'xgb']
dataset = 'adult'
init_samples(dataset)
get_model_samples(dataset, models)


#%%
get_all_rules('gender=F', models, dataset)


#%%
models =['xgb']
dataset = 'adult'
get_model_samples(dataset, models)
get_all_rules('race=Female', models, dataset)


#%%
get_all_rules('race=Black',['lr', 'xgb', 'knn', 'rf', 'dt', 'svm'], 'adult')


#%%
models =['gnb']
dataset = 'adult'
get_model_samples(dataset, models)

#%% [markdown]
# ### reject option 
# 
# a post processing method for removing algorithmic discrimination.  
# For details, please refer to **Decision Theory for Discrimination-Aware Classification, 10.1109/ICDM.2012.45**

#%%
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.metrics import accuracy_score

def predict_post(x_samples, model, theta, samples, group, includes, excludes):
    y_samples = model.predict(x_samples)
    y_probs = model.predict_proba(x_samples)
    y_post_samples = []
    
    
    
    for idx, prob in enumerate( y_probs):
        is_include = any(            (all([ include[1] in samples.iloc[idx][include[0]]                   for include in include_group]))             for include_group in includes)         or len(includes)==0
        
        is_exclude = all(            (all([ exclude[1] not in samples.iloc[idx][exclude[0]]                   for exclude in exclude_group]))             for exclude_group in excludes)             or len(excludes)==0
        
        
        # if in the reject option space, modify label
        if max(prob[1], 1- prob[1])<theta         and is_include        and is_exclude:
            if group[1] in samples.iloc[idx][group[0]]:
                y_post_samples.append(1)
            else: 
                y_post_samples.append(0)
        # else, remain the same
        else:
            y_post_samples.append(y_samples[idx])
    return y_post_samples
        

def run_reject_option(model_name, dataset_name, theta, group=['gender', 'M'], includes=[[]], excludes=[[]], context=True):
#     f = open('./cache/models/model_{}_{}.pkl'.format(model_name, dataset_name), 'rb')    
#     model = pickle.load(f)  
    
    if os.path.isfile('{}_mdlp.pkl'.format(dataset_name)):
        f=open('{}_mdlp.pkl'.format(dataset_name), 'rb')
        mdlp = dill.load(f)
    else:
        raise Exception('no mdlp exists, run init_samples first')
        
    #  training data
    dataset_path = '../server/{}.csv'.format(dataset_name)
    data = pd.read_csv(dataset_path)
    encoder = DataEncoder()
    encoder.fit(num2cate_transform(data, mdlp))
    x, y = encoder.transform(num2cate_transform(data, mdlp))
    
#     score_cross = cross_val_score(model, x, y, scoring='accuracy', cv = KFold(n_splits=5, shuffle = True)) 
#     print('cross valide', score_cross)

    
#     load the discriminatory region
    if context:
        f = open(store_path + '{}_{}_default_region.json'.format(model_name, dataset_name),'r')
        discri_region = json.load(f)
        discri_region = [[a.split('=') for a in i['antecedent']] for i in discri_region]
        includes_ = includes + discri_region 
    else:
        includes_ = includes
    
    k =5
    scores = []
    kf = KFold(n_splits=k)
    kf.get_n_splits(x)

    for train_index, test_index in kf.split(x):
        x_train, x_test = x[train_index], x[test_index]
        y_train, y_test = y[train_index], y[test_index]
        
        model_gene = ModelGene('{}_{}'.format(dataset_name,model_name))
        model = model_gene.model
        
        model.fit(x_train, y_train)
        score = accuracy_score(y_test, model.predict(x_test))
        scores.append(score)
    print ('score of {}'.format(model_name), sum(scores)/len(scores))
    
    scores = []
    for train_index, test_index in kf.split(x):
        x_train, x_test = x[train_index], x[test_index]
        y_train, y_test = y[train_index], y[test_index]
        
        model_gene = ModelGene('{}_{}'.format(dataset_name,model_name))
        model = model_gene.model
        model.fit(x_train, y_train)
        
        y_post_test = predict_post(x_test, model, theta, num2cate_transform(data, mdlp), group, includes_, excludes)
        score = accuracy_score(y_test, y_post_test)
        scores.append(score)
    print ('score after roc of {}'.format(model_name), sum(scores)/len(scores))
        
#     #  for synthetic data, save samples for rules mining
#     model_gene = ModelGene('{}_{}'.format(dataset_name,model_name))
#     model = model_gene.model
#     model.fit(x, y)
    
    # load model from the memory to avoid randomness
    f = open('./cache/models/model_{}_{}.pkl'.format(model_name, dataset_name), 'rb')    
    model = pickle.load(f) 
    
        
    samples_path = os.path.join(store_path, '{}_samples.json'.format(dataset_name))
    samplesInit = pd.read_json(samples_path)
    samples = num2cate_transform(samplesInit, mdlp)
    x_samples, _ = encoder.transform(samples)
    y_samples_post = predict_post(x_samples, model, theta, samples, group, includes_, excludes)
    
    #  concate post processing result to samples
    num_samples = samplesInit.copy()
    num_samples['class'] = pd.Series(np.asarray(y_samples_post), index= samples.index) 

    cate_samples = samples.copy()
    cate_samples['class'] = pd.Series(np.asarray(y_samples_post), index= samples.index) 
    
    
    # add the ID col 
    cate_samples.insert(loc=0, column='id', value= cate_samples.index)
    num_samples.insert(loc=0, column='id', value= num_samples.index)

    # save mdeol & samples to cache
    dataOut = pd.concat([num_samples,cate_samples])

    samples_path = os.path.join(store_path, '{}_{}_post{}{}_samples.json'.format(dataset_name, model_name, len(includes), len(excludes)))
    dataOut.to_json(samples_path, orient='records')

    samples_path = os.path.join(store_path, '{}_{}_post{}{}_samples.csv'.format(dataset_name, model_name, len(includes), len(excludes)))
    dataOut.to_csv(samples_path, index=False)
    

#%% [markdown]
# ## random forest

#%%
# rf_post00 0.78
# run_reject_option('rf', 'academic', 0.8, ['gender', 'M'], [], [])
# get_all_rules('gender=F', ['rf_post00'], 'academic')


# rf_post31 0.90
run_reject_option('rf', 'academic', 0.8,                   ['gender', 'M'],                   [],                  []
                 )
get_all_rules('gender=F', ['rf_post00'], 'academic')

# # rf_post11 0.89
# run_reject_option('rf', 'academic', 0.8, \
#                   ['gender', 'M'], \
#                   [\
#                    [['raisedhands', '50']], \
#                   ],\
#                   [\
#                    [['Semester', 'S']]
#                   ]
#                  )


# # rf_post21 0.90
# run_reject_option('rf', 'academic', 0.8, \
#                   ['gender', 'M'], \
#                   [\
#                    [['raisedhands', '50']], \
#                    [['AnnouncementsView', '66']], \
#                   ],\
#                   [\
#                    [['Semester', 'S']]
#                   ]
#                  )

# rf_post41 0.89
# run_reject_option('rf', 'academic', 0.8, \
#                   ['gender', 'M'], \
#                   [\
#                    [['raisedhands', '50']], \
#                    [['raisedhands', '23']], \
#                    [['AnnouncementsView', '66']], \
#                    [['StudentAbsenceDays','Under-7']]  \
#                   ],\
#                   [\
#                    [['Semester', 'S']]
#                   ]
#                  )

#%% [markdown]
# ## KNN

#%%
# run_reject_option('knn', 'academic', 0.8, [['gender', 'M'], ['StudentAbsenceDays', 'Above-7']])
# run_reject_option('knn', 'academic', 0.8, [['gender', 'M'],  ['VisITedResources', '15']])


# # knn_post00 0.860
context = True
run_reject_option('knn', 'academic', 0.8, ['gender', 'M'], [], [], context)
get_all_rules('gender=F', ['knn_post00'], 'academic')




# # knn_post41 0.9
# run_reject_option('knn', 'academic', 0.8, \
#                   ['gender', 'M'], \
#                   [\
#                    [['raisedhands', '50']], \
#                    [['raisedhands', '23']], \
#                    [['AnnouncementsView', '66']], \
#                    [['StudentAbsenceDays','Under-7']]  \
#                   ],\
#                   [\
#                    [['StudentAbsenceDays','Above-7'], ['raisedhands', 'x>50'], ['AnnouncementsView', '10'], ['Relation','Father']]
#                   ]
#                  )

# # # knn_post11 0.875
# run_reject_option('knn', 'academic', 0.8, \
#                   ['gender', 'M'], \
#                   [\
#                    [['StudentAbsenceDays','Above-7']],  \
#                    [['raisedhands', '23']],\
#                    [['AnnouncementsView', '20']]\
#                   ],\
#                   [\
# #                    [ ['AnnouncementsView', '10']]\
#                    [['raisedhands', '50'], ['AnnouncementsView', '10']]\
#                   ]
#                  )

# get_all_rules('gender=F', ['knn_post31'], 'academic')

#%% [markdown]
# ## Decision Tree

#%%
#  dt_post0 original reject option, 0.808
# dt_post00 context reject option

run_reject_option('dt', 'academic', 0.9,                   ['gender', 'M'],                   [],                  [],                  True
                 )
get_all_rules('gender=F', ['dt_post00'], 'academic')


#%%
any([[] for i in []])


#%%



